{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math \n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from numpy import linalg as LA\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from operator import itemgetter \n",
    "from scipy.stats import kendalltau\n",
    "import pandas as pd\n",
    "import math\n",
    "from numpy import linalg as LA\n",
    "\n",
    "G=nx.Graph()\n",
    "\n",
    "f = open(\"bio-dmela.txt\", \"r\")\n",
    "edge1=[]\n",
    "edge2=[]\n",
    "nodelist=[]\n",
    "for x in f:\n",
    "  e1,e2=x.split()\n",
    "  edge1.append(int(e1))\n",
    "  edge2.append(int(e2))\n",
    "  for i in (int(e1),int(e2)):\n",
    "      if i not in nodelist:\n",
    "          nodelist.append(i)\n",
    "nodelist.sort()\n",
    "t=0\n",
    "nnodelist=[]\n",
    "dic={}\n",
    "for i in nodelist:\n",
    "    dic[i]=t\n",
    "    nnodelist.append(dic[i])\n",
    "    t=t+1\n",
    "\n",
    "nodelist=nnodelist\n",
    "for  i in range(0, len(edge1)):\n",
    "    G.add_edge(dic[edge1[i]], dic[edge2[i]])\n",
    "\n",
    "nedge1=[]\n",
    "nedge2=[]\n",
    "for  i in range(0, len(edge1)):\n",
    "   nedge1.append(dic[edge1[i]])\n",
    "   nedge2.append(dic[edge2[i]])\n",
    "   \n",
    "edge1=nedge1\n",
    "edge2=nedge2\n",
    "        \n",
    "n=len(nodelist)\n",
    "d=[0]*n\n",
    "for i in  range(0,n):\n",
    "    d[i]=edge1.count(i)+edge2.count(i)\n",
    "D={node:val for (node, val) in G.degree()}\n",
    "# B=nx.betweenness_centrality(G)\n",
    "# C=nx.closeness_centrality(G)\n",
    "# CC=nx.clustering(G)\n",
    "# G1 = nx.DiGraph(G)\n",
    "# pr = nx.pagerank(G1, alpha=0.85)\n",
    "# #print(pr)\n",
    "\n",
    "# L = nx.normalized_laplacian_matrix(G)\n",
    "# e = np.linalg.eigvals(L.A)\n",
    "# #print(\"Largest eigenvalue:\", max(e))\n",
    "# #phi = (1 + math.sqrt(max(e))) / 2.0\n",
    "# kc= nx.katz_centrality(G,0.01)\n",
    "# #print(kc)\n",
    "def LGC(v, alpha):\n",
    "    sum=0\n",
    "    for u in nodelist:\n",
    "        if v!=u:\n",
    "            d_ij=nx.shortest_path_length(G, source=u, target=v)\n",
    "            #print(d_ij)\n",
    "            #print(D[u])\n",
    "            sum=sum+(math.sqrt(D[u]+alpha))/d_ij\n",
    "    return sum*(D[v]/n)\n",
    "\n",
    "lgc={}\n",
    "alpha=0.5\n",
    "for v in nodelist:\n",
    "    lgc[v]=LGC(v,alpha)\n",
    "\n",
    "\n",
    "def get_key(dictionay, val):\n",
    "    for key, value in dictionay.items():\n",
    "         if val == value:\n",
    "             return key\n",
    "\n",
    "def get_vertices(List):\n",
    "    if type(List)==list:\n",
    "        res = sorted(range(len(List)), key = lambda sub: List[sub])\n",
    "        return res\n",
    "    if type(List)==dict:\n",
    "        res = dict(sorted(List.items(), key = itemgetter(1), reverse = True)) \n",
    "        return res\n",
    "\n",
    "# #for Degree centrality values and vertices\n",
    "# vertices_dd=get_vertices(D)\n",
    "# vertices_d=[]\n",
    "# values_d=[]\n",
    "# for i in vertices_dd:\n",
    "#     vertices_d.append(i)\n",
    "#     values_d.append(vertices_dd[i])\n",
    "# #vertices_cc.reverse()\n",
    "# #values_cc.reverse()\n",
    "# print(\"vertices of D \", vertices_d)\n",
    "# print(\"value of D: \", values_d)\n",
    "\n",
    "# #for betweenness values and vertices   \n",
    "# vertices_bwd=get_vertices(B)\n",
    "# vertices_bw=[]\n",
    "# values_bw=[]\n",
    "# for i in vertices_bwd:\n",
    "#     vertices_bw.append(i)\n",
    "#     values_bw.append(vertices_bwd[i])\n",
    "# #vertices_bw.reverse()\n",
    "# #values_bw.reverse()\n",
    "# print(\"vertices of bw \", vertices_bw)\n",
    "# print(\"value of bw: \", values_bw)\n",
    "\n",
    "\n",
    "# #for closeness centrality values and vertices\n",
    "# vertices_ccd=get_vertices(C)\n",
    "# vertices_closec=[]\n",
    "# values_closec=[]\n",
    "# for i in vertices_ccd:\n",
    "#     vertices_closec.append(i)\n",
    "#     values_closec.append(vertices_ccd[i])\n",
    "# #vertices_cc.reverse()\n",
    "# #values_cc.reverse()\n",
    "# print(\"vertices of closec \", vertices_closec)\n",
    "# print(\"value of closec: \", values_closec)\n",
    "\n",
    "\n",
    "# #for clustering coefficent centrality values and vertices\n",
    "# vertices_clustcd=get_vertices(CC)\n",
    "# vertices_clustc=[]\n",
    "# values_clustc=[]\n",
    "# for i in vertices_clustcd:\n",
    "#     vertices_clustc.append(i)\n",
    "#     values_clustc.append(vertices_clustcd[i])\n",
    "# #vertices_cc.reverse()\n",
    "# #values_cc.reverse()\n",
    "# print(\"vertices of clustc \", vertices_clustc)\n",
    "# print(\"value of clustc: \", values_clustc)\n",
    "\n",
    "\n",
    "\n",
    "# #for page rank values and vertices\n",
    "# vertices_pr=get_vertices(pr)\n",
    "# vertices_p=[]\n",
    "# values_p=[]\n",
    "# for i in vertices_pr:\n",
    "#     vertices_p.append(i)\n",
    "#     values_p.append(vertices_pr[i])\n",
    "# #vertices_cc.reverse()\n",
    "# #values_cc.reverse()\n",
    "# print(\"vertices of pr \", vertices_p)\n",
    "# print(\"value of pr: \", values_p)\n",
    "\n",
    "\n",
    "# #for Katz Centrality values and vertices\n",
    "# vertices_kc=get_vertices(kc)\n",
    "# vertices_k=[]\n",
    "# values_k=[]\n",
    "# for i in vertices_kc:\n",
    "#     vertices_k.append(i)\n",
    "#     values_k.append(vertices_kc[i])\n",
    "# #vertices_cc.reverse()\n",
    "# #values_cc.reverse()\n",
    "# print(\"vertices of kc \", vertices_k)\n",
    "# print(\"value of kc: \", values_k)\n",
    "\n",
    "\n",
    "#for lgc centrality values and vertices\n",
    "\n",
    "vertices_lgc=get_vertices(lgc)\n",
    "vertices_l=[]\n",
    "values_l=[]\n",
    "for i in vertices_lgc:\n",
    "    vertices_l.append(i)\n",
    "    values_l.append(vertices_lgc[i])\n",
    "#vertices_cc.reverse()\n",
    "#values_cc.reverse()\n",
    "print(\"vertices of lgc \", vertices_l)\n",
    "print(\"value of lgc: \", values_l)\n",
    "\n",
    "print(\"Top-10 Vertices: \",vertices_l[:10])\n",
    "print(\"Top-10 Values: \",values_l[:10])\n",
    "\n",
    "\n",
    "# n=10\n",
    "# x=[]\n",
    "# LGC_=[]\n",
    "# #print(len(G.nodes))\n",
    "# for i in range(0,round(len(G.nodes())*0.75),n):\n",
    "#     new_G=nx.Graph()\n",
    "#     new_G = G.copy()\n",
    "#     new_G.remove_nodes_from(vertices_l[:i])\n",
    "#     #print(G.nodes)\n",
    "#     #print(\"Gnodes\",len(G.nodes))\n",
    "#     #print(\"newGnodes\",len(new_G.nodes))\n",
    "#     x.append(i)\n",
    "#     LGC_.append(nx.number_connected_components(new_G))\n",
    "#     #print(i)\n",
    "# #print(x)\n",
    "# #print(ISC_D)\n",
    "# #print(len(nodes[:i]))\n",
    "\n",
    "# df=pd.DataFrame(list(zip(x,LGC_)),columns=['No_of_Nodes_Removed','No_of_Connected_Components'])\n",
    "# print(df)\n",
    "# df.to_excel('FB_LGC_No_Connected_Components.xlsx','Sheet1')\n",
    "\n",
    "# df1=pd.DataFrame(list(zip(values_l,vertices_l)),columns=['Values','Vertices'])\n",
    "# print(df1)\n",
    "# df1.to_excel('FB_LGC_VN.xlsx','Sheet1')\n",
    "\n",
    "\n",
    "\n",
    "# def IC(g,S,p=0.5,mc=1000):\n",
    "#     \"\"\"\n",
    "#     Input:  graph object, set of seed nodes, propagation probability\n",
    "#             and the number of Monte-Carlo simulations\n",
    "#     Output: average number of nodes influenced by the seed nodes\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Loop over the Monte-Carlo Simulations\n",
    "#     spread = []\n",
    "#     for i in range(mc):\n",
    "        \n",
    "#         # Simulate propagation process      \n",
    "#         new_active, A = S[:], S[:]\n",
    "#         while new_active:\n",
    "\n",
    "#             # For each newly active node, find its neighbors that become activated\n",
    "#             new_ones = []\n",
    "#             for node in new_active:\n",
    "                \n",
    "#                 # Determine neighbors that become infected\n",
    "#                 np.random.seed(i)\n",
    "#                 success = np.random.uniform(0,1,len([x for x in g.neighbors(node)])) < p\n",
    "#                 new_ones += list(np.extract(success, [x for x in g.neighbors(node)]))\n",
    "\n",
    "#             new_active = list(set(new_ones) - set(A))\n",
    "            \n",
    "#             # Add newly activated nodes to the set of activated nodes\n",
    "#             A += new_active\n",
    "            \n",
    "#         spread.append(len(A))\n",
    "        \n",
    "#     return(np.mean(spread))\n",
    "\n",
    "# # ISdc=IC(G,vertices_d[:2])\n",
    "# # print(ISdc)\n",
    "# # ISbw=IC(G,vertices_bw[:2])\n",
    "# # print(ISbw)\n",
    "# # ISc=IC(G,vertices_closec[:2])\n",
    "# # print(ISc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
